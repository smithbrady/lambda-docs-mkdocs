{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-lambda-docs","title":"Welcome to Lambda Docs","text":"<p>Warning</p> <p>As of March 1, 2024, Lambda charges sales tax in jurisdictions it's required to do so. Keep your billing address up to date to ensure the tax is correctly calculated and to avoid disruption to your account.</p> <p>Note</p> <p>As of March 4, 2024, Cloud file systems in the Texas, USA (us-south-1) region are billed the same as file systems in other regions. See our FAQ to learn more about how file systems are billed.</p>"},{"location":"#whats-new-in-april-2024","title":"What's new in April 2024","text":"<ul> <li>Lambda's ML Team released ML Times, an LLM-powered app that aggregates and   summarizes AI and ML content from various sources such as the   r/MachineLearning subreddit and Hacker News.</li> </ul>"},{"location":"#whats-new-in-march-2024","title":"What's new in March 2024","text":"<ul> <li>Beginning March 1, Lambda started charging sales tax for On-Demand Cloud.</li> <li>Beginning March 4, Lambda started billing for usage of Cloud file systems in   the Texas, USA (us-south-1) region.</li> <li>virtiofsd for On-Demand Cloud was upgraded. This upgrade fixes an issue that   caused some customers to encounter \"Too many open files\" errors, which could   also lead to Jupyter Notebook becoming unresponsive.</li> </ul>"},{"location":"#top-faqs","title":"Top FAQs","text":""},{"location":"#gpu-cloud","title":"GPU Cloud","text":"<ul> <li>Can I launch an instance from the command line?</li> <li>Why is my credit or debit card being declined?</li> <li>How do I get started using the Team feature?</li> <li>How do I terminate an instance using the Cloud API?</li> <li>How do I use persistent storage to save datasets and system state?</li> <li>Can I pause my instance instead of terminating it?</li> </ul>"},{"location":"1-click-clusters/getting-started/","title":"Getting started","text":"<p>Example</p> <p>Hello World!</p>","tags":["distributed training"]},{"location":"1-click-clusters/getting-started/#conceptual-overview","title":"Conceptual overview","text":"<p>1-Click Clusters (1CC) are clusters of GPU and CPU instances in Lambda On-Demand Cloud consisting of 16 to 512 NVIDIA H100 SXM Tensor Core GPUs. Compute (GPU) nodes are interconnected over an NVIDIA Quantum-2 400Gb/s InfiniBand non-blocking fabric in a rail-optimized topology, providing peer-to-peer GPUDirect RDMA communication of up to 3200Gb/s. All nodes are connected via 2x100Gb/s Ethernet for IP communication and are connected to the Internet via 2x100Gb/s Direct Internet Access (DIA) circuits.</p> <p>Each 1CC includes 3x CPU management (head) nodes for use as jump boxes (bastion hosts) and for cluster administration and job scheduling. These management nodes are assigned public IP addresses and are directly accessible over the Internet via SSH.</p> <p>All nodes can be directly accessed using Jupyter Notebook from the Lambda On-Demand Cloud dashboard.</p> <p>1CC nodes are in an isolated private network and can communicate freely with each other using private IP addresses.</p> <p>Generic CPU nodes can optionally be launched in the same regions as 1CCs. These generic CPU nodes run independently of 1CCs and don't terminate when 1CC reservations end.</p> <p>Each compute node includes 24TB of usable local ephemeral NVMe storage. Each management node includes 208GB of usable local ephemeral NVMe storage. Persistent storage is automatically created and attached to each 1CC node, and can also be attached to on-demand instances. Existing file systems in the same region can additionally be attached. You're billed only for the storage you actually use.</p> <p>All 1CC nodes are preinstalled with Ubuntu 22.04 LTS and Lambda Stack, including NCCL, Open MPI, PyTorch with DDP and FSDP support, TensorFlow, OFED, and other popular libraries and frameworks for distributed ML workloads, allowing ML engineers and researchers to begin their large-scale experiments and other work immediately after launching a 1CC.</p>","tags":["distributed training"]},{"location":"includes/hello-world.incl/","title":"Hello world.incl","text":"<p>Example</p> <p>Hello World!</p>"},{"location":"on-demand-cloud/billing/","title":"Billing","text":""},{"location":"on-demand-cloud/billing/#how-are-on-demand-instances-billed","title":"How are on-demand instances billed?","text":"<p>Before you can launch on-demand instances, you need to add a credit card to your account using the dashboard. We'll make a $10 pre-authorization charge to make sure the card is valid, similar to how gas stations and hotels do. The charge will be refunded in a few days.</p> <p>On-demand instances are billed in one-minute increments from the moment you spin up (start) the instance up to the moment you terminate (stop) the instance.</p> <p>Danger</p> <p>Be sure to terminate any instances that you're not using!</p> <p>You will be billed for all minutes that an instance is running, even if the instance isn't actively being used.</p> <p>The GPU Cloud dashboard allows you to view your resource usage.</p> <p>Invoices are sent weekly for the previous week's usage.</p> <p>Note</p> <p>On-demand instances require us to maintain excess capacity at all times so we can meet the changing workloads of our customers. For this reason, on-demand instances are priced higher than reserved instances.</p> <p>Conversely, we offer reserved GPU Cloud instances at a significant savings over on-demand instances, since they allow us to more accurately determine our capacity needs ahead of time.</p>"},{"location":"on-demand-cloud/getting-started/","title":"Getting started","text":"","tags":["kubernetes"]},{"location":"on-demand-cloud/getting-started/#can-my-data-be-recovered-once-ive-terminated-my-instance","title":"Can my data be recovered once I've terminated my instance?","text":"<p>Danger</p> <p>We cannot recover your data once you've terminated your instance! Before terminating an instance, make sure to back up all data that you want to keep.</p> <p>If you want to save data even after you terminate your instance, create a persistent storage file system.</p> <p>Note</p> <p>The persistent storage file system must be attached to your instance before you start your instance. The file system cannot be attached to your instance after you start your instance.</p> <p>When you create a file system, a directory with the name of your file system is created in your home directory. For example, if the name of your file system is PERSISTENT-FILE-SYSTEM, the directory is created at <code>/home/ubuntu/PERSISTENT-FILE-SYSTEM</code>. Data not stored in this directory is erased once you terminate your instance and cannot be recovered.</p>","tags":["kubernetes"]},{"location":"on-demand-cloud/getting-started/#can-i-pause-my-instance-instead-of-terminating-it","title":"Can I pause my instance instead of terminating it?","text":"<p>It currently isn't possible to pause (suspend) your instance rather than terminating it. But, this feature is in the works.</p> <p>Until this feature is implemented, you can use persistent storage file systems to imitate some of the benefits of being able to pause your instance.</p>","tags":["kubernetes"]},{"location":"on-demand-cloud/getting-started/#do-you-support-kubernetes-k8s","title":"Do you support Kubernetes (K8s)?","text":"<p>Kubernetes, also known as K8s, isn't supported on On-Demand Cloud.</p> <p>However, Lambda offers managed Kubernetes for Reserved Cloud.</p> <p>See our Managed Kubernetes Product Outline to learn more.</p>","tags":["kubernetes"]},{"location":"on-demand-cloud/getting-started/#why-cant-my-program-find-the-nvidia-cudnn-library","title":"Why can't my program find the NVIDIA cuDNN library?","text":"<p>Unfortunately, the NVIDIA cuDNN license limits how cuDNN can be used on our instances.</p> <p>On our instances, cuDNN can only be used by the PyTorch\u00ae framework and TensorFlow library installed as part of Lambda Stack.</p> <p>Other software, including PyTorch and TensorFlow installed outside of Lambda Stack, won't be able to find and use the cuDNN library installed on our instances.</p> <p>Tip</p> <p>Software outside of Lambda Stack usually looks for the cuDNN library files in <code>/usr/lib/x86_64-linux-gnu</code>. However, on our instances, the cuDNN library files are in <code>/usr/lib/python3/dist-packages/tensorflow</code>.</p> <p>Creating symbolic links, or \"symlinks,\" for the cuDNN library files might allow your program to find the cuDNN library on our instances.</p> <p>Run the following command to create symlinks for the cuDNN library files:</p> <pre><code>for cudnn_so in /usr/lib/python3/dist-packages/tensorflow/libcudnn*; do\n  sudo ln -s \"$cudnn_so\" /usr/lib/x86_64-linux-gnu/\ndone\n</code></pre>","tags":["kubernetes"]},{"location":"on-demand-cloud/getting-started/#how-do-i-upgrade-python","title":"How do I upgrade Python?","text":"<p>Danger</p> <p>Upgrading Python, that is, replacing the preinstalled Python version with a newer version, will break your instance.</p> <p>Instead of upgrading Python, you should install your desired version of Python alongside the preinstalled version, and use your desired version in a virtual environment.</p> <ol> <li>To install another version of Python alongside the preinstalled version:</li> </ol> <p>Run <code>sudo apt -y update &amp;&amp; sudo apt -y install pythonVERSION-full</code>.</p> <p>Replace VERSION with the Python version you want to install, for    example, <code>3.13</code>. Make sure <code>-full</code> is appended to the Python version,    otherwise, you won't have the <code>venv</code> module needed to create Python virtual    environments.</p> <p>As a complete example, if you want to install Python version 3.13, run:</p> <pre><code>sudo apt -y update &amp;&amp; sudo apt -y install python3.13-full\n</code></pre> <ol> <li>Run <code>pythonVERSION -m venv VENV-NAME</code> to create a Python virtual    environment.</li> </ol> <p>Replace VERSION with the Python version you installed in the previous    step. Replace VENV-NAME with the name you want to give your virtual    environment.</p> <p>Then, run <code>source VENV-NAME/bin/activate</code>.</p> <p>Replace VENV-NAME with the name you gave your virtual environment.</p> <p>As a complete example, if you want to create a virtual environment named    <code>my-virtual-environment</code> using Python version 3.13 (installed in the    example in the previous step), run:</p> <pre><code>python3.13 -m venv my-virtual-environment\nsource my-virtual-environment/bin/activate\n</code></pre> <p>Run <code>python --version</code> to confirm that your virtual environment is using    your desired Python version.</p>","tags":["kubernetes"]},{"location":"software/virtual-environments-containers/","title":"Virtual environments and Docker containers","text":""},{"location":"software/virtual-environments-containers/#what-are-virtual-environments","title":"What are virtual environments?","text":"<p>Virtual environments allow you to create and maintain development environments that are isolated from each other. Lambda recommends using either:</p> <ul> <li>Python venv</li> <li>conda</li> </ul>"},{"location":"software/virtual-environments-containers/#creating-a-python-virtual-environment","title":"Creating a Python virtual environment","text":"<ol> <li>Create a Python virtual environment using the <code>venv</code> module by running:</li> </ol> <pre><code>python -m venv --system-site-packages NAME\n</code></pre> <p>Replace NAME with the name you want to give to your virtual environment.</p> <p>Note</p> <p>The command, above, creates a virtual environment that has access to Lambda Stack packages and packages installed from Ubuntu repositories.</p> <p>To create a virtual environment that doesn't have access to Lambda Stack and Ubuntu packages, omit the <code>--system-site-packages option</code>.</p> <ol> <li>Activate the virtual environment by running:</li> </ol> <pre><code>source NAME/bin/activate\n</code></pre> <p>Replace NAME with the name you gave your virtual environment in the previous step.</p> <p>Python packages you install in your virtual environment are isolated from the base environment and other virtual environments.</p> <p>Note</p> <p>Locally installed packages can conflict with packages installed in virtual environments. For this reason, it's recommended to uninstall locally installed packages by running:</p> <p>To uninstall packages installed locally for your user only, run:</p> <pre><code>pip uninstall -y $(pip -v list | grep ${HOME}/.local | awk '{printf \"%s \", $1}')\n</code></pre> <p>To uninstall packages installed locally, system-wide (for all users), run:</p> <pre><code>sudo pip uninstall -y $(pip -v list | grep /usr/local | awk '{printf \"%s \", $1}')\n</code></pre> <p>Danger</p> <p>Don't run the above uninstall commands on Lambda GPU Cloud on-demand instances!</p> <p>The above uninstall commands remove all locally installed packages and, on on-demand instances, break programs including pip and JupyterLab.</p>"},{"location":"workstations/getting-started/","title":"Getting started","text":""},{"location":"workstations/getting-started/#how-do-i-set-up-my-workstation","title":"How do I set up my workstation?","text":"<p>Instructions for setting up your Vector can be found in our Vector quickstart guide.</p> <p>Instructions for setting up your Vector One can be found in our Vector One quickstart guide.</p>"},{"location":"workstations/getting-started/#what-are-the-buttons-and-ports-at-the-front-and-top-of-my-vector-one","title":"What are the buttons and ports at the front and top of my Vector One?","text":"Vector One front and top buttons and ports <p>Your Vector One's power button is located at the front-top.</p> <p>The first button at the top, closest to the front, switches between the various RGB modes. The second button at the top changes the color.</p> <p>The first port at the top, closest to the front, is used to connect USB-C 3.1 devices. The following 2 ports are used to connect USB-A 3.0 devices.</p> <p>The jack at the top is used to connect a headset/microphone.</p>"},{"location":"workstations/getting-started/#how-do-i-fix-wi-fi-issues-with-my-vector-one","title":"How do I fix Wi-Fi issues with my Vector One?","text":"<p>There are known issues in Ubuntu with the Wi-Fi adapter installed in Vector Ones. In some cases, the Wi-Fi adapter isn't detected at all. In other cases, the Wi-Fi adapter is detected but exhibits slow performance. These issues are fixed in updated firmware for the Wi-Fi adapter.</p> <p>In order to download and install the updated firmware, you need to connect your Vector One to the Internet.</p> <p>If your Wi-Fi adapter isn't detected at all, try booting using a previous kernel version. Your Wi-Fi adapter might be detected and you can download the updated firmware.</p> <p>Note</p> <p>You can also connect your Vector One to the Internet using Ethernet (recommended), a USB Wi-Fi adapter, or by tethering your iPhone or Android phone.</p> <p>Once your Vector One is connected to the Internet, open a terminal and run:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre> <p>Then, reboot your Vector One.</p>"},{"location":"workstations/getting-started/#where-can-i-download-recovery-images-for-my-workstation","title":"Where can I download recovery images for my workstation?","text":"<p>Workstation recovery images can be downloaded from our Lambda Stack and recovery images docs page.</p>"},{"location":"workstations/getting-started/#can-i-dual-boot-ubuntu-and-windows-11","title":"Can I dual boot Ubuntu and Windows 11?","text":"<p>You can't dual boot Ubuntu and Windows 11. You can have either Ubuntu or Windows installed, but not both at the same time on your workstation.</p> <p>Windows 11 requires Secure Boot and TPM 2.0 to be enabled, which can prevent Ubuntu from booting or detecting your workstation's GPUs.</p>"},{"location":"workstations/getting-started/#how-do-i-set-the-fan-speeds-for-my-workstation","title":"How do I set the fan speeds for my workstation?","text":"<p>You can set baseline fan speeds for your workstation using <code>ipmitool</code>. Once baseline fan speeds are set, you can fine-tune the fan speeds in the web-based IPMI interface.</p> <p>Note</p> <p>These instructions are only for workstations using an ASUS Pro WS WRX80E-SAGE SE WIFI motherboard.</p> <p>Before proceeding with these instructions, run <code>sudo dmidecode -t 2 | grep Name</code> to confirm your workstation uses the above motherboard. You should see: <code>Product Name: Pro WS WRX80E-SAGE SE</code>.</p> <p>First, install <code>ipmitool</code> by running:</p> <pre><code>sudo apt -y update &amp;&amp; sudo apt -y install ipmitool\n</code></pre> <p>Then, set the baseline fan speeds by running:</p> <pre><code>sudo ipmitool raw 0x30 0x0E 0x04 0x00 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x01 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x02 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x03 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x04 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x05 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x06 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64\n</code></pre> <p>Tip</p> <p>See the ASUS ASMB9-iKVM Fan Customized Mode User Guide [PDF] to learn how to customize fan speeds in the web-based IPMI interface.</p> <p>Note that Lambda workstations are high-performance systems and generate plenty of heat. For this reason, it's not recommended to use the guide's power efficiency fan policy.</p>"},{"location":"workstations/getting-started/#what-are-the-power-requirements-for-my-workstations-psu","title":"What are the power requirements for my workstation's PSU?","text":"<p>Danger</p> <p>Lambda workstations are high-performance systems and use a large amount of power. For this reason, Lambda workstations can't reliably be used with uninterruptible power supplies (UPSs, or battery backups).</p> <p>If you use a UPS with your workstation, you might experience system instability and trouble booting.</p> <p>The power requirements for Lambda workstation power supply units (PSUs) are as follows:</p> <p>Note</p> <p>The manufacturer and model of your workstation\u2019s PSU appears on the label on the PSU.</p> Manufacturer Model Wattage Voltage (AC) Current (A) Frequency (Hz) Inlet/Outlet Super Flower SF-1300F14MG V1.0 1300 100-240 15 60/50 C14/C13 Super Flower SF-1600F14HT 1600 115-240 17-10 60/50 C20/C19 Super Flower SF-2000F14HP 2000 200-240 15 50 C20/C19"},{"location":"workstations/getting-started/#how-do-i-upgrade-my-samsung-980-pro-nvme-ssds-firmware","title":"How do I upgrade my Samsung 980 PRO NVMe SSD's firmware?","text":"<p>Follow these instructions to upgrade your Samsung 980 PRO NVMe SSD's firmware.</p> <p>Danger</p> <p>Samsung 980 PRO NVMe SSDs with the older 3B2QGXA7 firmware are known to fail.</p> <p>To know if your SSD is using the 3B2QGXA7 firmware, install the <code>smartmontools</code> package by running <code>sudo apt -y install smartmontools</code>. Then, run <code>sudo smartctl -a /dev/nvme0</code>.</p> <p>If your SSD is using the 3B2QGXA7 firmware, it's recommended that you upgrade the firmware as soon as possible.</p> <p>First, download the latest firmware ISO from Samsung's website by running:</p> <pre><code>wget https://semiconductor.samsung.com/resources/software-resources/Samsung_SSD_980_PRO_5B2QGXA7.iso\n</code></pre> <p>Next, run <code>sudo -s</code> to open a shell with root (administrator) privileges.</p> <p>Finally, run:</p> <pre><code>mkdir /mnt/iso &amp;&amp; mount -o loop Samsung_SSD_980_PRO_5B2QGXA7.iso /mnt/iso &amp;&amp; \\\nmkdir fwupdate &amp;&amp; cd fwupdate &amp;&amp; \\\ngzip -dc /mnt/iso/initrd | cpio -idv --no-absolute-filenames &amp;&amp; \\\ncd root/fumagician &amp;&amp; ./fumagician\n</code></pre> <p>The above command mounts the firmware upgrade ISO, extracts the firmware upgrade, and launches the upgrade.</p> <p>After the firmware upgrade completes, restart your computer.</p> <p>Run <code>sudo smartctl -a /dev/nvme0</code> to confirm your SSD is using the new firmware.</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#distributed-training","title":"distributed training","text":"<ul> <li>Getting started</li> </ul>"},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>Getting started</li> </ul>"}]}